{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Bayesian Neural Network to classify MNIST Fashion Data set.\n",
    "'''\n",
    "The architecture is LeNet-5 [1].\n",
    "#### References\n",
    "[1]: Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.\n",
    "     Gradient-based learning applied to document recognition.\n",
    "     _Proceedings of the IEEE_, 1998.\n",
    "     http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython.core.debugger import  set_trace\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Pre-processing for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnother way of loading the data using Keras.\\nfashionData = tf.keras.datasets.fashion_mnist\\n(x_train, y_train), (x_test,y_test) = fashionData.load_data()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "# x_train are number of images. \n",
    "# y_train are the associated labels.\n",
    "# x_test are number of images. \n",
    "# y_test are the associated labels.\n",
    "'''\n",
    "Another way of loading the data using Keras.\n",
    "fashionData = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test,y_test) = fashionData.load_data()\n",
    "'''\n",
    "#fashion_mnist = input_data.read_data_sets('input/fashion',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting input/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting input/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting input/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting input/fashion/t10k-labels-idx1-ubyte.gz\n",
      "Number of Images for testing : 10000 \n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = input_data.read_data_sets('input/fashion',one_hot=True)\n",
    "print (\"Number of Images for testing : {test} \". format(test=fashion_mnist.test.num_examples))\n",
    "# 9 Labels Associated with each data-set\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "flag = False # Change it to true to see the data\n",
    "\n",
    "if flag:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i in range(64):\n",
    "        plt.subplot(8,8,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(fashion_mnist.train.images[i].reshape(28,28), cmap=plt.cm.binary)\n",
    "        print()\n",
    "        plt.xlabel(class_names[np.where(fashion_mnist.train.labels[i] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_pipeline(mnist_data,batch_size,validation_datasize):\n",
    "    \n",
    "    #set_trace()\n",
    "   \n",
    "    \n",
    "    Train = tf.reshape(mnist_data.train.images,[-1,28,28,1])\n",
    "    Validation = tf.reshape(mnist_data.validation.images,[-1,28,28,1])\n",
    "    \n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                        (Train, np.int32(mnist_data.train.labels)))\n",
    "    \n",
    "    training_batches = training_dataset.shuffle(50000,reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "    \n",
    "    training_iterator = training_batches.make_one_shot_iterator()\n",
    "\n",
    "    # Build a iterator over the heldout set with batch_size=heldout_size,\n",
    "    # i.e., return the entire heldout set as a constant.\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((Validation ,\n",
    "                                           np.int32(mnist_data.validation.labels)))\n",
    "    \n",
    "    validation_frozen = (validation_dataset.take(validation_datasize).repeat().batch(validation_datasize))\n",
    "    \n",
    "    validation_iterator = validation_frozen.make_one_shot_iterator()\n",
    "\n",
    "    # Combine these into a feedable iterator that can switch between training\n",
    "    # and validation inputs.\n",
    "    \n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    \n",
    "    feedable_iterator = tf.data.Iterator.from_string_handle(handle, training_batches.output_types, \n",
    "                                                            training_batches.output_shapes)\n",
    "    \n",
    "    images, labels = feedable_iterator.get_next()\n",
    "\n",
    "    return images, labels, handle, training_iterator, validation_iterator    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(batch_size,learning_rate,max_steps):\n",
    "    \n",
    "    # Load Fashion MNIST data \n",
    "    \n",
    "    fashion_mnist = input_data.read_data_sets('input/fashion',one_hot=False)\n",
    "    \n",
    "    #fashion_mnist.train.images = tf.reshape(fashion_mnist.train.images,[-1,28,28,1])\n",
    "    #fashion_mnist.validation.images = tf.reshape(fashion_mnist.validation.images,[-1,28,28,1])\n",
    "    \n",
    "    (images, labels, handle,training_iterator, heldout_iterator) = build_input_pipeline(\n",
    "       fashion_mnist, batch_size, fashion_mnist.validation.num_examples)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Build a Bayesian LeNet5 network. We use the Flipout Monte Carlo estimator\n",
    "    # for the convolution and fully-connected layers: this enables lower\n",
    "    # variance stochastic gradients than naive reparameterization.\n",
    "    with tf.name_scope(\"bayesian_neural_net\", values=[images]):\n",
    "        neural_net = tf.keras.Sequential([\n",
    "            tfp.layers.Convolution2DFlipout(6,kernel_size=5,padding=\"SAME\",activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=[2, 2],strides=[2, 2],padding=\"SAME\"),\n",
    "            tfp.layers.Convolution2DFlipout(16,kernel_size=5,padding=\"SAME\",activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=[2, 2],strides=[2, 2],padding=\"SAME\"),\n",
    "            tfp.layers.Convolution2DFlipout(120,kernel_size=5,padding=\"SAME\",activation=tf.nn.relu),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tfp.layers.DenseFlipout(84, activation=tf.nn.relu),\n",
    "            tfp.layers.DenseFlipout(10)\n",
    "        ])\n",
    "\n",
    "    logits = neural_net(images)\n",
    "    labels_distribution = tfd.Categorical(logits=logits)\n",
    "\n",
    "    # We Compute the -ELBO as the loss, averaged over the batch size\n",
    "    # To futher understand about ELBO or Variational Lower bound loss Look at the following reference below.\n",
    "    # http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf\n",
    "    \n",
    "    neg_log_likelihood = -tf.reduce_mean(labels_distribution.log_prob(labels))\n",
    "    kl = sum(neural_net.losses) / fashion_mnist.train.num_examples\n",
    "    elbo_loss = neg_log_likelihood + kl\n",
    "\n",
    "    # Build metrics for evaluation. Predictions are formed from a single forward\n",
    "    # pass of the probabilistic layers. They are cheap but noisy predictions.\n",
    "    \n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    accuracy, accuracy_update_op = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
    "    \n",
    "    # Extract weight posterior statistics for layers with weight distributions\n",
    "    # for later visualization.\n",
    "    names = []\n",
    "    qmeans = []\n",
    "    qstds = []\n",
    "    \n",
    "    for i, layer in enumerate(neural_net.layers):\n",
    "        try:\n",
    "            q = layer.kernel_posterior\n",
    "        except AttributeError:\n",
    "              continue\n",
    "        names.append(\"Layer {}\".format(i))\n",
    "        qmeans.append(q.mean())\n",
    "        qstds.append(q.stddev())\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(elbo_loss)\n",
    "        \n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                    tf.local_variables_initializer())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Run the training loop.\n",
    "        train_handle = sess.run(training_iterator.string_handle())\n",
    "        heldout_handle = sess.run(heldout_iterator.string_handle())\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            _ = sess.run([train_op, accuracy_update_op],feed_dict={handle: train_handle})\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                loss_value, accuracy_value = sess.run([elbo_loss, accuracy], feed_dict={handle: train_handle})\n",
    "                print(\"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f}\".format(step, loss_value, accuracy_value))\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting input/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting input/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting input/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting input/fashion/t10k-labels-idx1-ubyte.gz\n",
      "Step:   0 Loss: 27.807 Accuracy: 0.133\n",
      "Step: 100 Loss: 25.492 Accuracy: 0.521\n",
      "Step: 200 Loss: 24.583 Accuracy: 0.626\n",
      "Step: 300 Loss: 23.797 Accuracy: 0.674\n"
     ]
    }
   ],
   "source": [
    "main(batch_size= 128,learning_rate=0.001,max_steps= 400)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#For visualizing the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_monte_carlo = \n",
    "        # Compute log prob of heldout set by averaging draws from the model:\n",
    "        # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
    "        #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
    "        # where model_i is a draw from the posterior p(model|train).\n",
    "        probs = np.asarray([sess.run((labels_distribution.probs),\n",
    "                                     feed_dict={handle: heldout_handle})\n",
    "                            for _ in range(num_monte_carlo)])\n",
    "        mean_probs = np.mean(probs, axis=0)\n",
    "\n",
    "        image_vals, label_vals = sess.run((images, labels),\n",
    "                                          feed_dict={handle: heldout_handle})\n",
    "        heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]),\n",
    "                                               label_vals.flatten()]))\n",
    "        print(\" ... Held-out nats: {:.3f}\".format(heldout_lp))\n",
    "\n",
    "        qm_vals, qs_vals = sess.run((qmeans, qstds))\n",
    "\n",
    "    if HAS_SEABORN:\n",
    "        plot_weight_posteriors(names, qm_vals, qs_vals,\n",
    "                                 fname=os.path.join(\n",
    "                                     FLAGS.model_dir,\n",
    "                                     \"step{:05d}_weights.png\".format(step)))\n",
    "        plot_heldout_prediction(image_vals, probs,\n",
    "                                  fname=os.path.join(\n",
    "                                      FLAGS.model_dir,\n",
    "                                      \"step{:05d}_pred.png\".format(step)),\n",
    "                                  title=\"mean heldout logprob {:.2f}\"\n",
    "                                  .format(heldout_lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_4env",
   "language": "python",
   "name": "py3_4env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
